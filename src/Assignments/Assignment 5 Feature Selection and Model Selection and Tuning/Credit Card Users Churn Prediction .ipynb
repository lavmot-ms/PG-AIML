{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a477cd37",
   "metadata": {},
   "source": [
    "## <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\">Assignment 5: Credit Card Users Churn Prediction</span>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\" style=\"font-family: Arial;color:#0e92ea;font-size:1em;\">\n",
    "The Thera bank recently saw a steep decline in the number of users of their credit card, credit cards are a good source of income for banks because of different kinds of fees charged by the banks like annual fees, balance transfer fees, and cash advance fees, late payment fees, foreign transaction fees, and others. Some fees are charged to every user irrespective of usage, while others are charged under specified circumstances. Goal is to come up with a classification model that will help the bank improve its services so that customers do not renounce their credit cards\n",
    "</p>\n",
    "\n",
    "##  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> Contents:</span>\n",
    "\n",
    "<ol style=\"font-family: Arial;color:#0e92ea;font-size:1em;\">\n",
    "    <li>Exploratory Data Analysis and Insights</li>\n",
    "    <li>Data pre-processing</li>\n",
    "    <li>Model building</li>\n",
    "    <li>Model building - Oversampled data</li>\n",
    "    <li>Model building - Undersampled data</li>\n",
    "    <li>Hyperparameter tuning using random search</li>\n",
    "    <li>Model Performances</li>\n",
    "    <li>Productionize the model</li>\n",
    "    <li>Recommendations</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pip install category_encoders\n",
    "pip install xgboost\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help with reading and manipulation of data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "# To help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "\n",
    "# To split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# to create k folds of data and get cross validation score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# to create pipeline and make_pipeline\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# to use standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# To impute missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To build a Random forest classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# To tune a model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# To get different performance metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# To undersample and oversample the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# To suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "base_data = pd.read_csv('BankChurners.csv')\n",
    "\n",
    "default_color_palette  = [\"#03A9F4\",\"#7C4DFF\", \"#FF5252\", \"#D50000\", \"#FF6F00\", \"#0288D1\", \"#7C4DFF\"]\n",
    "maritalstatus_color_palette = {'Single':'#ff3d00', 'Married':'#00c853', 'Divorced': '#EA112F'}\n",
    "stats_colors           = {'Mean':'#D50000', 'Mode':'#FF3D00', 'Median':'#2962FF'} # Set standard colors for mean, mode and median to use accross entire notebook.\n",
    "gender_color_palette   = {\"F\":\"#E91E63\", \"M\":\"#42A5F5\"} # Gender based colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e6490",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 1 and 2: Exploratory Data Analysis and Insights and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "Pandas DataFrame\n",
    "\n",
    "Output:\n",
    "Displays DataFrame structure \n",
    "(columns, nulls and non nulls counts and percentage highlighing columns with most number of nulls)\n",
    "\n",
    "Retunrs:\n",
    "N/A\n",
    "'''\n",
    "def info(dataFrame):\n",
    "    print(f\"{dataFrame.shape[0]} Rows x {dataFrame.shape[1]} Columns\")\n",
    "    nulls_series      = dataFrame.isna().sum()         # Get a series counting number of empty values for each column\n",
    "    nonnulls_series   = dataFrame.notnull().sum()      # Get a series counting number of non empty valuesfor each column \n",
    "    nulls_percentage  = ((nulls_series * 100)/(nulls_series + nonnulls_series)).astype(float)\n",
    "    column_datatypes  = dataFrame.dtypes               # Get a series containing data types for each column \n",
    "    \n",
    "    series_arr    = [nulls_series, nonnulls_series, nulls_percentage, column_datatypes]\n",
    "    col_names_arr = [\"Nulls\", \"Non-Nulls\",\"Nulls %\", \"Type\"]\n",
    "    \n",
    "    nulls_count_df = pd.concat(\n",
    "        objs = series_arr,\n",
    "        axis = 1,\n",
    "        keys = col_names_arr, \n",
    "        sort = True)\n",
    "    \n",
    "    cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "    display(nulls_count_df.style.background_gradient(cmap=cm, subset=pd.IndexSlice[:, ['Nulls %']]).format(formatter={('Nulls %'): \"{:.2f}%\"})) \n",
    "    \n",
    "\n",
    "'''\n",
    "Input:\n",
    "Target type and a list of feature names.\n",
    "\n",
    "Output:\n",
    "Convert all features provided in 'column_names' to Target type provided in 'toType'\n",
    "\n",
    "Returns:\n",
    "modifies main original data frame and returns nothing.\n",
    "'''\n",
    "def ConvertColTo(toType = \"category\", column_names=np.nan, df = np.nan):\n",
    "    for col_name in column_names:\n",
    "        df[col_name] = df[col_name].astype(toType)\n",
    "\n",
    "\n",
    "'''\n",
    "Input:\n",
    "N/A\n",
    "\n",
    "Output:\n",
    "Go through each categorial column and print unique values for that column.\n",
    "\n",
    "Retunrs:\n",
    "N/A\n",
    "'''\n",
    "def CountUniqueValues(col_names):\n",
    "    for col_name in col_names:\n",
    "        print(f\"======================='{col_name}'==================\")\n",
    "        for unique_col_value in  main_data[col_name].unique().tolist():\n",
    "            total_count = main_data[col_name].count()\n",
    "            unique_values_count = main_data[main_data[col_name] == unique_col_value][col_name].count()\n",
    "            percentage = str(round((unique_values_count/total_count) * 100, 2))\n",
    "            print(f\"{unique_col_value} \\t: {unique_values_count} ({percentage}%)\")\n",
    "        print(f\"=========================================================\\n\")\n",
    "        \n",
    "'''\n",
    "Input:\n",
    "list of column names\n",
    "\n",
    "Output:\n",
    "a dictionary contating coloumn with unique value counts\n",
    "split by thhe threshold\n",
    "\n",
    "Retunrs:\n",
    "N/A\n",
    "'''\n",
    "def SplitColumnsByUniqueValueCountThreshold(col_names, threshold = 30):\n",
    "    results = {\n",
    "        'lower_set' : [],\n",
    "        'upper_set': []\n",
    "    }\n",
    "    \n",
    "    for col_name in col_names:            \n",
    "        if len(list(main_data[col_name].unique())) < threshold:\n",
    "            results['lower_set'].append(col_name)\n",
    "        else:\n",
    "            results['upper_set'].append(col_name)\n",
    "            \n",
    "    pprint.pprint(results)\n",
    "    return results\n",
    "\n",
    "'''\n",
    "Description:\n",
    "Given the model the test data y values, displays the confusion matrix for that model.\n",
    "\n",
    "Input:\n",
    "model    - The learning model.\n",
    "y_actual - Y values from test data.\n",
    "labels   - the labels\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "'''\n",
    "def DisplayConfusionMatrix(model, y_actual, labels=[1,0]):\n",
    "    y_predict             = model.predict(x_test)\n",
    "    confusion_matrix      = metrics.confusion_matrix(y_actual, y_predict, labels=[0, 1])\n",
    "    \n",
    "    confusion_matix_df = pd.DataFrame(\n",
    "        data    = confusion_matrix,\n",
    "        index   = [i for i in [\"Actual No\", \"Actual Yes\"]],\n",
    "        columns = [i for i in [\"Predicted - No\", \"Predicted - Yes\"]])\n",
    "    \n",
    "    group_counts     = [\"{0:0.0f}\".format(value) for value in confusion_matrix.flatten()]\n",
    "    group_percetages = [\"{0:.2%}\".format(value) for value in confusion_matrix.flatten()/np.sum(confusion_matrix)]\n",
    "    \n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percetages)]\n",
    "    labels = np.array(labels).reshape(2, 2)\n",
    "    plt.figure(figsize= (10,7))\n",
    "    sns.heatmap(confusion_matix_df, annot=labels,fmt='')\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "\n",
    "'''\n",
    "Description:\n",
    "Given the model, computes models perfomance on test and training data.\n",
    "Perf metrics displayed are: Accuracy, Recall, F1 Score, and precision.\n",
    "\n",
    "Input:\n",
    "model - The learning model.\n",
    "\n",
    "Returns:\n",
    "A dictionary containing models perfomace.\n",
    "'''\n",
    "def GetMetricsScore(model, x_train_arg, x_test_arg, y_train_arg, y_test_arg):\n",
    "    \n",
    "    pred_train = model.predict(x_train_arg)\n",
    "    pred_test  = model.predict(x_test_arg)\n",
    "    \n",
    "    train_accuracy = model.score(x_train_arg, y_train_arg)\n",
    "    test_accuracy  = model.score(x_test_arg, y_test_arg)\n",
    "    \n",
    "    train_recall = metrics.recall_score(y_train_arg, pred_train)\n",
    "    test_recall  = metrics.recall_score(y_test_arg, pred_test)\n",
    "    \n",
    "    train_precision = metrics.precision_score(y_train_arg, pred_train)\n",
    "    test_precision  = metrics.precision_score(y_test_arg, pred_test)\n",
    "\n",
    "    f1_score_train = 2 * ((train_precision * train_recall)/(train_precision + train_recall))\n",
    "    f1_score_test = 2 * ((test_precision * test_recall)/(test_precision + test_recall))\n",
    "    \n",
    "    return {\n",
    "        'Accuracy_Test'   : test_accuracy,\n",
    "        'Accuracy_Train'  : train_accuracy,\n",
    "        'Recall_Test'     : test_recall,\n",
    "        'Recall_Train'    : train_recall,\n",
    "        'Precision_Test'  : test_precision,\n",
    "        'Precision_Train' : train_precision,\n",
    "        'F1_Score_Train'  : f1_score_train,\n",
    "        'F1_Score_Test'   : f1_score_test\n",
    "    }\n",
    "        \n",
    "'''\n",
    "Description:\n",
    "Given the model, displays the importance score.\n",
    "\n",
    "Input:\n",
    "model - The learning model.\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "'''\n",
    "def DisplayImportanceScores(model):\n",
    "    gini_importances = pd.DataFrame(\n",
    "        data    = model.feature_importances_, \n",
    "        columns = [\"Imp\"],\n",
    "        index   = x_train.columns).sort_values(by = \"Imp\")\n",
    "    \n",
    "    print(gini_importances)\n",
    "\n",
    "'''\n",
    "Description:\n",
    "Given the model, displays the importance chart.\n",
    "\n",
    "Input:\n",
    "model    - The learning model.\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "'''\n",
    "def DisplayImportanceChart(model):\n",
    "    importances = model.feature_importances_\n",
    "    indices     = np.argsort(importances)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.title('Feature Importance')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "    plt.yticks(range(len(indices)), [list(X.columns)[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Description:\n",
    "Given the decision tree model, displays the decision tree of that model.\n",
    "\n",
    "Input:\n",
    "model - The learning model.\n",
    "size  - Size of the plot\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "'''\n",
    "def PlotDecisionTree(model, size= [20, 30]):\n",
    "    plt.figure(figsize=(size[0],size[1]))\n",
    "    tree.plot_tree(\n",
    "        model, \n",
    "        feature_names=feature_names,\n",
    "        filled=True,\n",
    "        fontsize=9,\n",
    "        node_ids=True,\n",
    "        class_names=True)\n",
    "    plt.show()\n",
    " \n",
    "'''\n",
    "Description:\n",
    "Given a model, displays the coefient values of that model\n",
    "\n",
    "Input:\n",
    "model - The learning model.\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "'''\n",
    "def DisplayCoeficients(model):\n",
    "    coefs = pd.DataFrame(\n",
    "        np.append(model.coef_, model.intercept_),\n",
    "        index=x_train.columns.tolist() + [\"Intercept\"],\n",
    "        columns=[\"Coefficients\"],\n",
    "    )\n",
    "\n",
    "    coefs.sort_values('Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee7c48",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff13558",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab09c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'Attrition_Flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "info(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.drop(\"CLIENTNUM\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_struct = {\n",
    "    \"Attrition_Flag\" : {\"Existing Customer\": 0, \"Attrited Customer\": 1},\n",
    "}\n",
    "base_data = base_data.replace(replace_struct)\n",
    "base_data[TARGET_COLUMN] = pd.to_numeric(base_data[TARGET_COLUMN], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data into X and Y\n",
    "X = base_data.drop([TARGET_COLUMN], axis = 1)\n",
    "Y = base_data[TARGET_COLUMN]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=1)\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea04adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = pd.concat([x_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b823f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52de977",
   "metadata": {},
   "source": [
    "### Missing Values Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935efa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "catgry_col_names = main_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvertColTo('category', catgry_col_names, main_data)\n",
    "info(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "catgry_col_names = main_data.select_dtypes(include=['category']).columns.tolist()\n",
    "CountUniqueValues(catgry_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb54448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MissingValuesTreatment(df):\n",
    "    df['Education_Level'].fillna(df['Education_Level'].value_counts().idxmax(), inplace=True)\n",
    "    df['Marital_Status'].fillna(df['Marital_Status'].value_counts().idxmax(), inplace=True)\n",
    "    \n",
    "MissingValuesTreatment(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "info(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountUniqueValues(catgry_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2082746",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "'''\n",
    "Description:\n",
    "Displays a grid catplots\n",
    "\n",
    "Input:\n",
    "A list of column names\n",
    "'''\n",
    "def DisplayCountPlotGrid(col_names, hue_name, color_palette):\n",
    "    col_index = 0\n",
    "    for r in range(0, int(len(col_names)), 3):\n",
    "        fig, axs = plt.subplots(\n",
    "            nrows=1,                                                                # Number of rows of the grid\n",
    "            ncols=3,                                                                # Number of columns of the grid.\n",
    "            figsize=(15,4),\n",
    "            constrained_layout=True)\n",
    "\n",
    "        for index in range(0, 3):\n",
    "            if col_index < int(len(col_names)):\n",
    "                column_name = col_names[col_index]\n",
    "                ax = axs.flat[index]\n",
    "                ax = sns.countplot(\n",
    "                    data=main_data,\n",
    "                    x=main_data[column_name],\n",
    "                    palette=color_palette,\n",
    "                    hue=hue_name,\n",
    "                    ax = ax)\n",
    "                ax.set_xlabel(column_name)                                           \n",
    "                ax.set_title(column_name + ' and '+ hue_name +' Profile', fontsize=14)\n",
    "                if int(len(ax.get_xticklabels())) > 14: \n",
    "                    ax.set_xticklabels([], rotation=45, ha='right')\n",
    "                else:\n",
    "                    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            col_index += 1\n",
    "            \n",
    "'''\n",
    "Input:\n",
    "N/A\n",
    "\n",
    "Output:\n",
    "Plot box plots in line.\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "'''     \n",
    "def PlotLineOfCountPlots(col_names, hue_name, palette_name):\n",
    "    fig, axs = plt.subplots(\n",
    "        len(col_names),\n",
    "        figsize = (15,6),\n",
    "        sharex  = False,\n",
    "        sharey  = False)\n",
    "    \n",
    "    fig.subplots_adjust(top = 4)\n",
    "    \n",
    "    for i in range(len(col_names)):\n",
    "        column_name = col_names[i]\n",
    "        sns.countplot(\n",
    "            data    = main_data, \n",
    "            x       = column_name,\n",
    "            hue     = hue_name,\n",
    "            palette = palette_name,\n",
    "            ax      = axs[i])\n",
    "        \n",
    "        axs[i].set_xlabel(column_name)                                           \n",
    "        axs[i].set_title(column_name + ' Profile', fontsize=14)\n",
    "        \n",
    "        if int(len(axs[i].get_xticklabels())) > 30: \n",
    "            axs[i].set_xticklabels([], rotation=45, ha='right')\n",
    "        else:\n",
    "            axs[i].set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_dic = SplitColumnsByUniqueValueCountThreshold(\n",
    "    main_data.select_dtypes(include=['float', 'int64']).columns.tolist(),\n",
    "    40)\n",
    "descrete_data_columns        = cols_dic['lower_set']\n",
    "large_descrete_data_columns  = cols_dic['upper_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9023de",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_descrete_data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09942e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descrete_data_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c123d91",
   "metadata": {},
   "source": [
    "### Exploring Univariate and Bivariate Analysis based on Gender\n",
    "- The number of clients is higher in customers who are Graduates\n",
    "- The number of clients is higher in customers who are Married and Single\n",
    "- The number of clients is higher in customers who earn less that $40k per year\n",
    "- Blue card has the highest number of clients and seems to be the one brining in more money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86802ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "catgry_col_names.append(TARGET_COLUMN)\n",
    "print(catgry_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names     = catgry_col_names,\n",
    "    hue_name      = \"Gender\",\n",
    "    color_palette = gender_color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fc680",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names    = descrete_data_columns,\n",
    "    hue_name     = \"Gender\",\n",
    "    color_palette = gender_color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c713b",
   "metadata": {},
   "source": [
    "### Exploring Univariate and Bivariate Analysis based on  Marital Status\n",
    "- The charts sho a higher number of clients who are married and single\n",
    "- Also the ratio of clients who left the service based on marital statis seems insignificance. Cannot deduce from the current of the data who category is most likely to stay or leave.\n",
    "- Another noticeable difference is how dominant maried category is.\n",
    "- Also most of the customers seems to be having a 2-3 dependents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf36cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountUniqueValues(['Marital_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names    = catgry_col_names,\n",
    "    hue_name     = \"Marital_Status\",\n",
    "    color_palette = maritalstatus_color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e78ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names       = descrete_data_columns,\n",
    "    hue_name        = \"Marital_Status\",\n",
    "    color_palette   = maritalstatus_color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775e0f3",
   "metadata": {},
   "source": [
    "### Exploring Univariate and Bivariate Analysis based on Education\n",
    "- Not that much noticeable differen except that most of the customers are graduates and they are also the ones most likely to leave the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountUniqueValues(['Education_Level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names    = catgry_col_names,\n",
    "    hue_name     = \"Education_Level\",\n",
    "    color_palette = default_color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names       = descrete_data_columns,\n",
    "    hue_name        = \"Education_Level\",\n",
    "    color_palette   = default_color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4dcd7f",
   "metadata": {},
   "source": [
    "### Exploring Univariate and Bivariate Analysis based on Card Type\n",
    "- Again we see blue as the most dominant card in all categories.\n",
    "- This blue card is also the one with the highest number of attrited customers.\n",
    "- There is also a higher number of customers using the blue card who are earning less than $40k per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c20974",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountUniqueValues(['Card_Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names    = catgry_col_names,\n",
    "    hue_name     = \"Card_Category\",\n",
    "    color_palette = default_color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names       = descrete_data_columns,\n",
    "    hue_name        = \"Card_Category\",\n",
    "    color_palette   = default_color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c6ecc",
   "metadata": {},
   "source": [
    "### Exploring Univariate and Bivariate Analysis based on Income\n",
    "- We notice a spike or peak values in customers who are earning less than \\$40k per year\n",
    "- The number of people leaving the credit card service based on income seems level or average accross all categories except for customers who are earning less than \\$40k per year with the highest number of attrition\n",
    "- Customers who earn less than \\$40k per year also have 2-3 dependants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c77c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountUniqueValues(['Income_Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names    = catgry_col_names,\n",
    "    hue_name     = \"Income_Category\",\n",
    "    color_palette = default_color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc455c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCountPlotGrid(\n",
    "    col_names       = descrete_data_columns,\n",
    "    hue_name        = \"Income_Category\",\n",
    "    color_palette   = default_color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121118c",
   "metadata": {},
   "source": [
    "### Data Processing and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColumnTransformation(df):\n",
    "    replace_struct = {\n",
    "        \"Card_Category\"       : {\"Blue\": 1, \"Silver\": 2, \"good\": 3, \"Gold\": 4, \"Platinum\": 5},\n",
    "        \"Income_Category\"     : {\"abc\": -1, \"Less than $40K\": 1, \"$40K - $60K\": 2, \"$60K - $80K\": 3, \"$80K - $120K\": 4, \"$120K +\": 5},\n",
    "        \"Education_Level\"     : {\"Uneducated\": 1, \"High School\": 2, \"College\": 3, \"Graduate\": 4, \"Post-Graduate\":5, \"Doctorate\": 6},\n",
    "        \"Attrition_Flag\"      : {\"Existing Customer\": 0, \"Attrited Customer\": 1},\n",
    "    }\n",
    "\n",
    "    df = df.replace(replace_struct)\n",
    "    df.head(10)\n",
    "    return df\n",
    "    \n",
    "main_data = ColumnTransformation(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ba4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoding(df):\n",
    "    oneHotCols = [\"Gender\", \"Marital_Status\"]\n",
    "    df = pd.get_dummies(df, columns=oneHotCols)\n",
    "    info(df)\n",
    "    return df\n",
    "\n",
    "main_data = OneHotEncoding(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixTargetFeatureType(df):\n",
    "    df[TARGET_COLUMN] = pd.to_numeric(df[TARGET_COLUMN], downcast=\"integer\")\n",
    "    return df\n",
    "    \n",
    "main_data = FixTargetFeatureType(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110eacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "info(main_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c7dc3",
   "metadata": {},
   "source": [
    "### Corrolation and Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,18)) \n",
    "sns.heatmap(data=main_data.corr().sort_values(by=[TARGET_COLUMN]), annot=True, linewidths=.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "Axis, column name/x-axis, hue name\n",
    "\n",
    "Output:\n",
    "Displays a count plot.\n",
    "\n",
    "Retunrs:\n",
    "N/A\n",
    "''' \n",
    "def HistBoxplot(box_chart_ax, hist_chart_ax, x_axis):\n",
    "    sns.boxplot(\n",
    "        data=main_data,\n",
    "        x=main_data[x_axis],\n",
    "        showmeans=True,\n",
    "        ax=box_chart_ax)\n",
    "    \n",
    "    sns.histplot(\n",
    "        data=main_data,\n",
    "        x=main_data[x_axis],\n",
    "        kde=True,\n",
    "        ax=hist_chart_ax)\n",
    "    \n",
    "    hist_chart_ax.axvline(main_data[x_axis].mean(),                 # Get the mean of the values in the given column and draw a vertical line that cuts the chart on the mean value \n",
    "               color=stats_colors['Mean'],                                  # Use on of the colors predefined on this notebook\n",
    "               label='Mean',                                                # Set the label to be diplayed on the legend\n",
    "               linestyle=\"dashed\");                                         # Make the line have dashes\n",
    "    \n",
    "    hist_chart_ax.axvline(main_data[x_axis].median(),               # Plot the median line on the chart.\n",
    "               color=stats_colors['Median'],                                # Use on of the colors predefined on this notebook\n",
    "               label='Median',                                              # Set the label to be diplayed on the legend\n",
    "               linestyle=\"dashed\");                                         # Make the line have dashes\n",
    "    \n",
    "    hist_chart_ax.axvline(main_data[x_axis].mode()[0],              # Plot the mode line on the chart.\n",
    "               color=stats_colors['Mode'],                                  # Use on of the colors predefined on this notebook \n",
    "               label='Mode',                                                # Set the label to be diplayed on the legend\n",
    "               linestyle=\"dashed\");                                         # Make the line have dashes\n",
    "    \n",
    "    hist_chart_ax.legend(loc='upper right')\n",
    "\n",
    "'''\n",
    "Input:\n",
    "N/A\n",
    "\n",
    "Output:\n",
    "Displays a a grid of [Boxplot x Distribution chart] for discrete features.\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "''' \n",
    "def PlotHistBoxGrid():\n",
    "    col_names = main_data.select_dtypes(include=['float', 'int64']).columns.tolist()\n",
    "    print(col_names)\n",
    "\n",
    "    col_index = 0\n",
    "    for r in range(0, int(len(col_names)), 3):\n",
    "        fig, (box, hist) = plt.subplots(\n",
    "            nrows=2,                                                                # Number of rows of the grid\n",
    "            ncols=3,                                                                # Number of columns of the grid.\n",
    "            figsize=(15,4),\n",
    "            gridspec_kw={\"height_ratios\" : (0.25,0.5)},\n",
    "            constrained_layout=True)\n",
    "\n",
    "        for index in range(0, 3):\n",
    "            if col_index < int(len(col_names)):\n",
    "                HistBoxplot(box.flat[index], hist.flat[index], col_names[col_index])\n",
    "            col_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "Column name\n",
    "\n",
    "Output:\n",
    "A series containing interquatile range values\n",
    "\n",
    "Retunrs:\n",
    "A dictionary containing quatile range values\n",
    "''' \n",
    "def Get_IQR(data):\n",
    "    quartiles = np.quantile(data, [.25, .75])\n",
    "    iqr = (quartiles[1] - quartiles[0])\n",
    "    #print(f'Q1 = {quartiles[0]}, Q3 = {quartiles[1]}, IQR = {iqr}')\n",
    "    return {\n",
    "        \"Q1\": quartiles[0],\n",
    "        \"Q3\": quartiles[1],\n",
    "        \"IQR\": iqr\n",
    "    }\n",
    "\n",
    "'''\n",
    "Description:\n",
    "replace outlier value with a given value (replacement_value) and an IQR multiplier (scale)\n",
    "\n",
    "Input:\n",
    "x                  - value to replace\n",
    "replacement_value  - value to replace the outlier with.\n",
    "quatiles           - dictionary containing quatile ranges {\"Q1\": X, \"Q2\": Y, \"IQR\": Z}\n",
    "scale.             - IQR multiplier\n",
    "\n",
    "Returns:\n",
    "new outlier value\n",
    "'''\n",
    "def ReplaceOutlier(x, replacement_value, quatiles, scale):\n",
    "    if x < quatiles[\"Q1\"] - scale * quatiles[\"IQR\"]  or x > quatiles[\"Q3\"] + scale * quatiles[\"IQR\"]:\n",
    "        return replacement_value\n",
    "    return x\n",
    "\n",
    "'''\n",
    "Description:\n",
    "Iterates through the IQR multiplier, to find the multiplier that\n",
    "yeild best corrolation with target variable\n",
    "'''\n",
    "def BestCorrOutlierTreatment(feature_column, target, df):\n",
    "    max_corr = 0.0\n",
    "    best_feature = df[feature_column]\n",
    "\n",
    "    for r in range(10):\n",
    "        main_data_copy = df.copy()\n",
    "        main_data_copy[feature_column] = main_data_copy[feature_column].apply(\n",
    "            ReplaceOutlier,\n",
    "            convert_dtype     = True,\n",
    "            replacement_value = main_data_copy[feature_column].median(),\n",
    "            scale             = r, \n",
    "            quatiles          = Get_IQR(main_data_copy[feature_column]))\n",
    "        \n",
    "        current_corr = main_data_copy.corr()[feature_column][target]\n",
    "         # print(f\"IQR Scale: {r}, Scale Corrolation : {current_corr}\")\n",
    "        \n",
    "        if current_corr > max_corr:\n",
    "            max_corr = current_corr\n",
    "            best_feature = main_data_copy[feature_column]\n",
    "            \n",
    "    for r in range(10):\n",
    "        main_data_copy = df.copy()\n",
    "        main_data_copy[feature_column] = main_data_copy[feature_column].apply(\n",
    "            ReplaceOutlier,\n",
    "            convert_dtype     = True,\n",
    "            replacement_value = main_data_copy[feature_column].mean(),\n",
    "            scale             = r, \n",
    "            quatiles          = Get_IQR(main_data_copy[feature_column]))\n",
    "        \n",
    "        current_corr = main_data_copy.corr()[feature_column][target]\n",
    "        \n",
    "        if current_corr > max_corr:\n",
    "            max_corr = current_corr\n",
    "            best_feature = main_data_copy[feature_column]\n",
    "            \n",
    "    return best_feature\n",
    "\n",
    "'''\n",
    "Input:\n",
    "N/A\n",
    "\n",
    "Output:\n",
    "Plot box plots in line.\n",
    "\n",
    "Returns:\n",
    "N/A\n",
    "''' \n",
    "def PlotBoxes():\n",
    "    columns = main_data.select_dtypes(include=['float', 'int64']).columns.tolist()\n",
    "    fig, axs = plt.subplots(len(columns), figsize=(15,10), sharex=False, sharey=False)\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=10, wspace=10, hspace=None)\n",
    "    for i in range(len(columns)):\n",
    "        column_name = columns[i]\n",
    "        sns.boxplot(data=main_data[column_name], orient=\"h\", palette=\"Set2\", ax = axs[i])\n",
    "        axs[i].set_xlabel(column_name)                                           \n",
    "        axs[i].set_title(column_name + ' Profile', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167973a8",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9091213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreatUoutliers(df):\n",
    "    columns = main_data.select_dtypes(include=['float', 'int64']).columns.tolist()\n",
    "\n",
    "    main_data_copy = df.copy()\n",
    "    for column in columns:\n",
    "        main_data_copy[column] = BestCorrOutlierTreatment(column, TARGET_COLUMN, main_data_copy)\n",
    "    \n",
    "    return main_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63302b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = main_data.corr()[[TARGET_COLUMN]].copy()\n",
    "df1[\"Tranformation\"] = \"Before\"\n",
    "\n",
    "df2 = TreatUoutliers(main_data).corr()[[TARGET_COLUMN]].copy()\n",
    "df2[\"Tranformation\"] = \"After\"\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "ax = sns.lineplot(data=df, x=\"index\", y=TARGET_COLUMN, hue=\"Tranformation\", ax=ax)\n",
    "ax.set(title='Changes in corrolation value after Outlier Treatment')\n",
    "ax.set(xlabel=None)\n",
    "plt.xticks(rotation = 45) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = TreatUoutliers(main_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589585f",
   "metadata": {},
   "source": [
    "### Pair Plot and Dropping Variables with high Variance and Very low corrolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef21c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with corrolation of less than 0.05 to the tagert variable\n",
    "list_of_features_to_drop = main_data.corr()[abs(main_data.corr()[TARGET_COLUMN]) < 0.05].index.to_list()\n",
    "main_data.drop(list_of_features_to_drop, axis=1, inplace=True)\n",
    "info(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7da9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(main_data, kind=\"reg\", hue=TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,18)) \n",
    "sns.heatmap(data=main_data.corr().sort_values(by=[TARGET_COLUMN]), annot=True, linewidths=.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41848829",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 4. Model Building and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = main_data.drop([TARGET_COLUMN], axis = 1)\n",
    "Y_2 = main_data[TARGET_COLUMN]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_2, Y_2, test_size=0.5, random_state=1)\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504657e",
   "metadata": {},
   "source": [
    "### Building Models with default values\n",
    "- The RandomForestClaffier model perfoms better (recall and F1 score) than all other models in most of the labs ran.\n",
    "- We select the RandomForest model to create the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ec451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildAndGetModels(x_var, y_var):\n",
    "    bagging_estimator=BaggingClassifier(random_state=1)\n",
    "    bagging_estimator.fit(x_var, y_var)\n",
    "    pprint.pprint(bagging_estimator)\n",
    "    \n",
    "    rf_estimator=RandomForestClassifier(random_state=1)\n",
    "    rf_estimator.fit(x_var, y_var)\n",
    "    pprint.pprint(rf_estimator)\n",
    "\n",
    "    logistic_reg_model = LogisticRegression()\n",
    "    logistic_reg_model.fit(x_var, y_var)\n",
    "    pprint.pprint(logistic_reg_model)\n",
    "\n",
    "    dtree1 = DecisionTreeClassifier(random_state=1)\n",
    "    dtree1.fit(x_var, y_var)\n",
    "    pprint.pprint(dtree1)\n",
    "\n",
    "    adaBoosting_Model = AdaBoostClassifier(random_state=1)\n",
    "    adaBoosting_Model.fit(x_var, y_var)\n",
    "    pprint.pprint(adaBoosting_Model)\n",
    "\n",
    "    gradientboost_model = GradientBoostingClassifier(random_state=1)\n",
    "    gradientboost_model.fit(x_var, y_var)\n",
    "    pprint.pprint(gradientboost_model)\n",
    "\n",
    "    models = {\n",
    "        'Bagging Classifier'        : bagging_estimator,\n",
    "        'RandomForest Model'        : rf_estimator,\n",
    "        'Logistic Regression Model' : logistic_reg_model,\n",
    "        'Decision Tree'             : dtree1,\n",
    "        'Gradient Boost'            : gradientboost_model,\n",
    "        'AdaBoostClassifier'        : adaBoosting_Model\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d89038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayCoefficients(models, x_var):\n",
    "    for model_name in models:        \n",
    "        coefs = pd.DataFrame(\n",
    "            np.append(models[model_name].coef_, models[model_name].intercept_),\n",
    "            index=x_var.columns.tolist() + [\"Intercept\"],\n",
    "            columns=[\"Coefficients\"],\n",
    "        )\n",
    "\n",
    "        print(coefs.sort_values('Coefficients'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ef65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetModelsScoreDataFrame(models, x_train_var, x_test_var, y_train_var, y_test_var):\n",
    "    scores = None\n",
    "    for model_name in models:\n",
    "        scores = GetMetricsScore(models[model_name], x_train_var, x_test_var, y_train_var, y_test_var)\n",
    "        print(f\"{model_name}\")\n",
    "    scores_overview_df = pd.DataFrame(columns=scores.keys())\n",
    "\n",
    "    for model_name in models:\n",
    "        scores = GetMetricsScore(models[model_name], x_train_var, x_test_var, y_train_var, y_test_var)\n",
    "        scores_overview_df.loc[model_name] = scores\n",
    "\n",
    "    return scores_overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445eb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = BuildAndGetModels(x_train, y_train)\n",
    "pprint.pprint(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e191c68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = GetModelsScoreDataFrame(models, x_train, x_val, y_train, y_val)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678ec92",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 5. Model building - Oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa50907",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy=0.4, k_neighbors=5, random_state=1)\n",
    "x_train_over, y_train_over = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "def BuildAndGetModels(x_var, y_var):\n",
    "    bagging_estimator=BaggingClassifier(random_state=1)\n",
    "    bagging_estimator.fit(x_var, y_var)\n",
    "    pprint.pprint(bagging_estimator)\n",
    "    \n",
    "    rf_estimator=RandomForestClassifier(random_state=1)\n",
    "    rf_estimator.fit(x_var, y_var)\n",
    "    pprint.pprint(rf_estimator)\n",
    "\n",
    "    logistic_reg_model = LogisticRegression()\n",
    "    logistic_reg_model.fit(x_var, y_var)\n",
    "    pprint.pprint(logistic_reg_model)\n",
    "\n",
    "    dtree1 = DecisionTreeClassifier(random_state=1)\n",
    "    dtree1.fit(x_var, y_var)\n",
    "    pprint.pprint(dtree1)\n",
    "\n",
    "    adaBoosting_Model = AdaBoostClassifier(random_state=1)\n",
    "    adaBoosting_Model.fit(x_var, y_var)\n",
    "    pprint.pprint(adaBoosting_Model)\n",
    "\n",
    "    gradientboost_model = GradientBoostingClassifier(random_state=1)\n",
    "    gradientboost_model.fit(x_var, y_var)\n",
    "    pprint.pprint(gradientboost_model)\n",
    "\n",
    "    models = {\n",
    "        'Bagging Classifier'        : bagging_estimator,\n",
    "        'RandomForest Model'        : rf_estimator,\n",
    "        'Logistic Regression Model' : logistic_reg_model,\n",
    "        'Decision Tree'             : dtree1,\n",
    "        'Gradient Boost'            : gradientboost_model,\n",
    "        'AdaBoostClassifier'        : adaBoosting_Model\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433559fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = BuildAndGetModels(x_train_over, y_train_over)\n",
    "pprint.pprint(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = GetModelsScoreDataFrame(models, x_train, x_val, y_train, y_val)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfad3a",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 6. Model building - Undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa18889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rus = RandomUnderSampler(random_state=1, sampling_strategy = 1)\n",
    "x_train_un, y_train_un = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "def BuildAndGetModels(x_var, y_var):\n",
    "    bagging_estimator=BaggingClassifier(random_state=1)\n",
    "    bagging_estimator.fit(x_var, y_var)\n",
    "    pprint.pprint(bagging_estimator)\n",
    "    \n",
    "    rf_estimator=RandomForestClassifier(random_state=1)\n",
    "    rf_estimator.fit(x_var, y_var)\n",
    "    pprint.pprint(rf_estimator)\n",
    "\n",
    "    logistic_reg_model = LogisticRegression()\n",
    "    logistic_reg_model.fit(x_var, y_var)\n",
    "    pprint.pprint(logistic_reg_model)\n",
    "\n",
    "    dtree1 = DecisionTreeClassifier(random_state=1)\n",
    "    dtree1.fit(x_var, y_var)\n",
    "    pprint.pprint(dtree1)\n",
    "\n",
    "    adaBoosting_Model = AdaBoostClassifier(random_state=1)\n",
    "    adaBoosting_Model.fit(x_var, y_var)\n",
    "    pprint.pprint(adaBoosting_Model)\n",
    "\n",
    "    gradientboost_model = GradientBoostingClassifier(random_state=1)\n",
    "    gradientboost_model.fit(x_var, y_var)\n",
    "    pprint.pprint(gradientboost_model)\n",
    "\n",
    "    models = {\n",
    "        'Bagging Classifier'        : bagging_estimator,\n",
    "        'RandomForest Model'        : rf_estimator,\n",
    "        'Logistic Regression Model' : logistic_reg_model,\n",
    "        'Decision Tree'             : dtree1,\n",
    "        'Gradient Boost'            : gradientboost_model,\n",
    "        'AdaBoostClassifier'        : adaBoosting_Model\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = BuildAndGetModels(x_train_un, y_train_un)\n",
    "pprint.pprint(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aff550",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = GetModelsScoreDataFrame(models, x_train, x_val, y_train, y_val)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1a32f",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 7. Hyperparameter tuning using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f720667",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_estimator=BaggingClassifier(random_state=1)\n",
    "bagging_estimator.fit(x_train, y_train)\n",
    "pprint.pprint(bagging_estimator)\n",
    "\n",
    "parameters = {\n",
    "    'max_samples'  : [0.7, 0.8, 0.9, 1],\n",
    "    'max_features' : [0.7, 0.8, 0.9, 1],\n",
    "    'n_estimators' : [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "grid_obj = RandomizedSearchCV(\n",
    "    estimator=bagging_estimator,\n",
    "    param_distributions=parameters,\n",
    "    n_jobs=-1,\n",
    "    n_iter=20,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "gird_obj = grid_obj.fit(x_train_over,y_train_over)\n",
    "bagging_estimator = grid_obj.best_estimator_\n",
    "bagging_estimator.fit(x_train_over,y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimator=RandomForestClassifier(random_state=1)\n",
    "rf_estimator.fit(x_train, y_train)\n",
    "pprint.pprint(rf_estimator)\n",
    "\n",
    "parameters = {\n",
    "    'class_weight'      : [{0: 0.3, 1: 0.7}],\n",
    "    'min_samples_leaf'  : np.arange(5, 10),\n",
    "    'max_features'      : np.arange(0.2,0.7, 0.1),\n",
    "    'max_samples'       : np.arange(0.3, 0.7, 0.1),\n",
    "    \"n_estimators\"      : [100, 150, 200, 250]\n",
    "}\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "grid_obj = RandomizedSearchCV(\n",
    "    estimator=rf_estimator,\n",
    "    param_distributions=parameters,\n",
    "    n_jobs=-1,\n",
    "    n_iter=20,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "gird_obj = grid_obj.fit(x_train_over,y_train_over)\n",
    "rf_estimator = grid_obj.best_estimator_\n",
    "rf_estimator.fit(x_train_over,y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree1 = DecisionTreeClassifier(random_state=1)\n",
    "dtree1.fit(x_train, y_train)\n",
    "pprint.pprint(dtree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774da877",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaBoosting_Model = AdaBoostClassifier(random_state=1)\n",
    "adaBoosting_Model.fit(x_train, y_train)\n",
    "pprint.pprint(adaBoosting_Model)\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\": np.arange(10, 110, 10),\n",
    "    \"learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n",
    "    \"base_estimator\": [\n",
    "        DecisionTreeClassifier(max_depth=1, random_state=1),\n",
    "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
    "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
    "    ],\n",
    "}\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "grid_obj = RandomizedSearchCV(\n",
    "    estimator=adaBoosting_Model,\n",
    "    param_distributions=parameters,\n",
    "    n_jobs=-1,\n",
    "    n_iter=20,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "gird_obj = grid_obj.fit(x_train_over,y_train_over)\n",
    "adaBoosting_Model = grid_obj.best_estimator_\n",
    "adaBoosting_Model.fit(x_train_over,y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientboost_model = GradientBoostingClassifier(random_state=1)\n",
    "gradientboost_model.fit(x_train, y_train)\n",
    "pprint.pprint(gradientboost_model)\n",
    "\n",
    "parameters = {\n",
    "    \"subsample\"       : [0.8, 0.9, 1],\n",
    "    \"n_estimators\"    : [100, 150, 250],\n",
    "    \"max_features\"    : [0.7, 0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "grid_obj = RandomizedSearchCV(\n",
    "    estimator=gradientboost_model,\n",
    "    param_distributions=parameters,\n",
    "    n_jobs=-1,\n",
    "    n_iter=20,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "gird_obj = grid_obj.fit(x_train_over,y_train_over)\n",
    "gradientboost_model = grid_obj.best_estimator_\n",
    "gradientboost_model.fit(x_train_over,y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Bagging Classifier'        : bagging_estimator,\n",
    "    'RandomForest Model'        : rf_estimator,\n",
    "    'Decision Tree'             : dtree1,\n",
    "    'Gradient Boost'            : gradientboost_model,\n",
    "    'AdaBoostClassifier'        : adaBoosting_Model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = GetModelsScoreDataFrame(models, x_train, x_val, y_train, y_val)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4b019",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 9. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a533f",
   "metadata": {},
   "source": [
    "### Custom Transformation Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e70cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "Target type and a list of feature names.\n",
    "\n",
    "Output:\n",
    "Convert all features provided in 'column_names' to Target type provided in 'toType'\n",
    "\n",
    "Returns:\n",
    "modifies main original data frame and returns nothing.\n",
    "'''      \n",
    "class ConvertObjectToCategoryType(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        catgry_col_names = x_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "        for col_name in catgry_col_names:\n",
    "            x_dataset[col_name] = x_dataset[col_name].astype(\"category\")\n",
    "        \n",
    "        return x_dataset\n",
    "    \n",
    "class FixOutliers(BaseEstimator):\n",
    "\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        columns = x_dataset.select_dtypes(include=['float', 'int64']).columns.tolist()\n",
    "        for column in columns:\n",
    "            x_dataset[column] = BestCorrOutlierTreatment(column, TARGET_COLUMN, pd.concat([x_dataset, pd.Series(self.y, name=TARGET_COLUMN)], axis=1).reset_index())\n",
    "        return x_dataset\n",
    "    \n",
    "class ReplaceMissingCatgrValues(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        x_dataset['Education_Level'].fillna(x_dataset['Education_Level'].value_counts().idxmax(), inplace=True)\n",
    "        x_dataset['Marital_Status'].fillna(x_dataset['Marital_Status'].value_counts().idxmax(), inplace=True)\n",
    "        return x_dataset\n",
    "    \n",
    "class ReplaceFeatureValuesWithIntValues(BaseEstimator):\n",
    "\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        replace_struct = {\n",
    "            \"Card_Category\"       : {\"Blue\": 1, \"Silver\": 2, \"good\": 3, \"Gold\": 4, \"Platinum\": 5},\n",
    "            \"Income_Category\"     : {\"abc\": -1, \"Less than $40K\": 1, \"$40K - $60K\": 2, \"$60K - $80K\": 3, \"$80K - $120K\": 4, \"$120K +\": 5},\n",
    "            \"Education_Level\"     : {\"Uneducated\": 1, \"High School\": 2, \"College\": 3, \"Graduate\": 4, \"Post-Graduate\":5, \"Doctorate\": 6},\n",
    "        }\n",
    "        x_dataset =  x_dataset.replace(replace_struct)\n",
    "        return x_dataset\n",
    "    \n",
    "class DropColumns(BaseEstimator):\n",
    "\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        df = pd.concat([x_dataset, self.y], axis=1).reset_index()\n",
    "        list_of_features_to_drop = df.corr()[abs(df.corr()[TARGET_COLUMN]) < 0.05].index.to_list()\n",
    "        df.drop(list_of_features_to_drop, axis=1, inplace=True)\n",
    "        x_dataset =  df.drop([TARGET_COLUMN], axis = 1)\n",
    "        return x_dataset\n",
    "    \n",
    "class DummyClassifier(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439434c",
   "metadata": {},
   "source": [
    "### Build The pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=1, sampling_strategy = 1)\n",
    "x_train_un, y_train_un = rus.fit_resample(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104a600",
   "metadata": {},
   "source": [
    "#### Build Pipeline on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f85775",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('Fix_Columns_Types', ConvertObjectToCategoryType()),\n",
    "    ('Missing_Values', ReplaceMissingCatgrValues()),\n",
    "    ('Column_Transformation', ReplaceFeatureValuesWithIntValues(y_test)),\n",
    "    ('OneHotEncoding', ce.OneHotEncoder(cols=[\"Gender\", \"Marital_Status\"], use_cat_names=True)),\n",
    "    ('Outlier_Treatment', FixOutliers(y_train_un)),\n",
    "    ('make_target_int', ConvertObjectToCategoryType()),\n",
    "    ('Drop_Features', DropColumns(y_train_un)),\n",
    "    ('clsf', RandomForestClassifier(random_state=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_un, y_train_un)\n",
    "pipeline.score(x_train_un, y_train_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964df1bc",
   "metadata": {},
   "source": [
    "#### Build Pipeline on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b96834",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('Fix_Columns_Types', ConvertObjectToCategoryType()),\n",
    "    ('Missing_Values', ReplaceMissingCatgrValues()),\n",
    "    ('Column_Transformation', ReplaceFeatureValuesWithIntValues(y_test)),\n",
    "    ('OneHotEncoding', ce.OneHotEncoder(cols=[\"Gender\", \"Marital_Status\"], use_cat_names=True)),\n",
    "    ('Outlier_Treatment', FixOutliers(y_test)),\n",
    "    ('make_target_int', ConvertObjectToCategoryType()),\n",
    "    ('Drop_Features', DropColumns(y_test)),\n",
    "    ('clsf', RandomForestClassifier(random_state=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc67224",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_test, y_test)\n",
    "pipeline.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0031ed6",
   "metadata": {},
   "source": [
    "####  <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> 10. Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cd791",
   "metadata": {},
   "source": [
    "- Attrition_Flag, Avg_Utilization_Ratio, Contacts_Count_12_mon, Months_Inactive_12_mon, Total_Ct_Chng_Q4_Q1, Total_Relationship_Count, Total_Revolving_Bal, Total_Trans_Amt, Total_Trans_Ct, these are the most important features to predict whether a customer will abondon the service or not.\n",
    "- The number of clients is higher in customers who are Graduates\n",
    "- The number of clients is higher in customers who are Married and Single\n",
    "- The number of clients is higher in customers who earn less that 40k per year\n",
    "- Blue card has the highest number of clients and seems to be the one brining in more money\n",
    "- The charts sho a higher number of clients who are married and single\n",
    "- Also the ratio of clients who left the service based on marital statis seems insignificance. Cannot deduce from the current of the data who category is most likely to stay or leave.\n",
    "- Another noticeable difference is how dominant maried category is.\n",
    "- Also most of the customers seems to be having a 2-3 dependents \n",
    "- The charts sho a higher number of clients who are married and single\n",
    "- Also the ratio of clients who left the service based on marital statis seems insignificance. Cannot deduce from the current of the data who category is most likely to stay or leave.\n",
    "- Another noticeable difference is how dominant maried category is.\n",
    "- Also most of the customers seems to be having a 2-3 dependents \n",
    "- Again we see blue as the most dominant card in all categories.\n",
    "- This blue card is also the one with the highest number of attrited customers.\n",
    "- There is also a higher number of customers using the blue card who are earning less than 40k per year.\n",
    "- We notice a spike or peak values in customers who are earning less than 40k per year\n",
    "- The number of people leaving the credit card service based on income seems level or average accross all categories except for customers who are earning less than 40k per year with the highest number of attrition\n",
    "- Customers who earn less than  40k per year also have 2-3 dependants\n",
    "- The RandomForestClaffier model perfoms better (recall and F1 score) than all other models in most of the labs ran.\n",
    "- We select the RandomForest model to create the pipeline.\n",
    "- Suggestions are:\n",
    "    - Look for programs to improve Avg_Utilization_Ratio\n",
    "    - The lower the Total_Revolving_Bal, the more chances of increeasing attrition\n",
    "    - decrease in Total_Trans_Ct means increase in retension, look for ways to help customers better use their cards less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbefbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
